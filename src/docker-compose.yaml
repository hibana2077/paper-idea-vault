services:
  web:
    build: ./web
    ports:
      - "80:80"
    volumes:
      - ./web:/app
    environment:
      USER: "admin"
      PASSWORD: "admin"
      BACKEND_SERVER: "backend:8081"
      OLLAMA_SERVER: "http://ollama:11434"
      LLM_PROVIDER: groq # or "ollama"
      LLM_MODEL: gemma2-9b-it # if LLM_PROVIDER is "ollama" then this should be the model name that available in the Ollama server
      LLM_API_TOKEN: "your-api-token-here"
    depends_on:
      - backend
    networks:
      - paper-idea-net

  backend:
    build: ./backend
    ports:
      - "8081:8081"
    volumes:
      - ./backend:/app
    environment:
      HOST: "0.0.0.0"
      OLLAMA_SERVER: "http://ollama:11434"
      REDIS_SERVER: "db"
      REDIS_PORT: 6379
    depends_on:
      - db
    networks:
      - paper-idea-net

  db:
    restart: always
    image: redis:latest
    ports:
      - "6379:6379"
    volumes:
      - ./data/redis-data:/data
    networks:
      - paper-idea-net
    
  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - "11434:11434"
  #   networks:
  #     - paper-idea-net
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [gpu]
  #   volumes:
  #     - ./ollama:/root/.ollama:rw

networks:
  paper-idea-net:
    driver: bridge